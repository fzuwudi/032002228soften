<a href="[Jiangleying/032002315: A painful programming experience (github.com)](https://github.com/Jiangleying/032002315)" title="超链接title">我的github作业链接</a>

#  一、PSP表格

| PSP2.1                                  | Personl Software Process Stages          | 预计耗时（分钟） | 实际耗时（分钟） |
| --------------------------------------- | ---------------------------------------- | ---------------- | ---------------- |
| Planning                                | 计划                                     |                  |                  |
| · Estimate                              | · 估计这个任务需要多少时间               | 30               | 20               |
| Development                             | 开发                                     |                  |                  |
| · Analysis                              | · 需求分析 (包括学习新技术）             | 240              | 360              |
| · Design Spec                           | ·  生成设计文档                          | 30               | 30               |
| · Design Review                         | · 设计复审                               | 20               | 30               |
| · Coding standard                       | · 代码规范 (为目前的开发制 定合适的规范) | 20               | 15               |
| · Design                                | ·  具体设计                              | 100              | 120              |
| · Coding                                | · 具体编码                               | 1000             | 1300             |
| · Code Review                           | · 代码复审                               | 90               | 80               |
| · Test                                  | · 测试（自我测试，修改代 码，提交修改）  | 60               | 60               |
| Reporting                               | 报告                                     |                  |                  |
| · Test Repor                            | · 测试报告                               | 60               | 40               |
| · Size Measurement                      | · 计算工作量                             | 10               | 10               |
| · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进 计划          | 60               | 45               |
|                                         | · 合计                                   | 1720             | 2110             |

# 二、计算模块接口

## 1. 项目设计与技术栈

> **技术栈**
>
> **基础**
>
> - Python
>
> **爬虫部分**
>
> - selenium
> - 正则表达式
>
> **前端部分**
>
> - Pyecharts
>
> **测试部分**
>
> - cProfile
>
> **项目设计**
>
> - python负责爬虫并将爬取到的数据放入excel
> - pyecharts负责将在excel中的数据进行可视化展示
>
> ------
>
> 1. 环节
> + 通过爬虫爬取卫健委每天更新的疫情通报网址
>
>   - 利用lxml通过网址进行html解析，获得每日疫情通报的文本数据
>    - 根据所获文本数据利用正则表达式及pandas将数据存储至excel表
>   - 利用pyecharts对excel表中的数据进行可视化处理
> 2. 渠道 && 方式
> + 渠道：b站速成网课、CSDN、博客园、大佬友情提供帮助
>
> - 爬虫部分：python所需库，现学现用
>
>   - 数据处理部分：pandas；通过pandas操作文档以及b站速成入门网课
>    - 可视化部分：pyecharts；利用网上已有的模板进行加工

## 2. 爬虫与数据处理

### 2.2.1：概述

+ 本项目实现了一个爬虫算法。对于爬取网页内容，我最先想到的便是从b站《3小时速成爬虫》学到的使用**requests模板**和**BeautifulSoup**进行网页的爬取及解析。但由于有些网页却有着讨厌的反爬机制，让我所学的知识毫无用武之地。

+ 而本次要爬取的卫健委官网便是这类讨厌的网站，在被一堆412错误砸到脸上后，我发现它的反爬机制具有以下几个特点：

> + 网页的cookie值是动态变化的，且大概十几秒会发生一次变化
> + 服务器在处理网络请求时需要验证cookie
> + 访问不同的页面会更换cookie，所以无法用一个cookie值爬取多个页面

+ 通过查阅资料，发现**selenium+火狐驱动**能够绕过反爬机制，这才成功捕获到网页内容。
+ 在分析网页文本时，最初我利用正则表达式直接试图从网页的源码提取需要的数据，但由于卫健委谨慎(善变)的网页变化，被时有时无的标签搞得欲哭无泪。于是转战把网页内容转化成文本化，就成功地从文本中正则出数据啦！

### 2.2.2：函数构成

```
def get_child_page()：获取每一天疫情报告的网址存在web_location中
def write_in_each_excel(all_dict,sfbt_dict,sfwzz_dict,gat_dict):将获取的数据存入excel
def get_detailed()：获取网页文本，并通过正则获取数据
```

> + 在get_child_page()获得每一天疫情报告的网址后，通过get_data.py文件访问每一天的网址将网页中的文本提取。第三个函数def get_detailed()为本part的主体函数，通过调用提取出的文本，先将文本转为可供匹配的形式，再利用正则匹配基于前后文找规律得到目标数据，最后调用def write_in_each_excel(all_dict,sfbt_dict,sfwzz_dict,gat_dict)将数据写入相应日期的excel表中。

+ 函数之间的关系如下图：

  ![image-20220915170157046](C:\Users\Ying\AppData\Roaming\Typora\typora-user-images\image-20220915170157046.png)

## 2.3 数据统计接口部分的性能改进

+ 使用python自带的cProfile库对get_data.py进行分析：

![avatar](https://img2022.cnblogs.com/blog/2288851/202209/2288851-20220919232055628-1572894995.png)

+ 通过测试可发现，通过爬取已爬下来的文本来获取数据的这一模块get_data耗时最大。

+ 因为代码中存在着for循环和sleep语句，因此跑的性能较差。

  

## 2.5 数据可视化界面的展示

> 为文件china_cov_19_2022_nogat.html

![avatar](https://img2022.cnblogs.com/blog/2288851/202209/2288851-20220919232041273-501393702.png)

+ 该动态化数据展示大屏，以右侧时间轴为变化趋势，可手动拖动到具体日期查看当日疫情情况；亦可自动播放，逐日自动展示每日疫情情况。
+ 页面中央为中国公鸡地图，以热度图的方式展示新增确诊/无症状人数的多少，不同的颜色代表不同人数区间，颜色越深代表该省相对于全国来说疫情的严重性。
+ 左侧可直观地看出每日各省的疫情状况和精确的疫情数据，并以每日新增确诊人数/无症状人数的最大值与最小值作为热度评判的High值与Low值，使展示更具有相对性。

![avatar](https://img2022.cnblogs.com/blog/2288851/202209/2288851-20220919232050308-2041965150.png)

+ 并且当鼠标移动到省份版图上时，会出现魔法！该版图居然会变成灰色，左边的热度也会精确指出该省份的数据！

![avatar](https://img2022.cnblogs.com/blog/2288851/202209/2288851-20220919232100655-2037765485.png)

+ 在新增了无症状数据后，我们可以通过最上面一行的按钮来切换每日新增确诊或新增无症状的人数可视化图。并新增了港澳台的数据。
+ 右下角也新增了饼图，使各省的人数关系更加直观。

# 三、心得

+ 最开始得知第一次个人编程主要是爬虫＋可视化的时候，我脑子中想到了之前寒假时实验班布置的爬虫作业，心中底气多了几分。

  因为不爱熬夜and赶ddl，我在初版pdf公布于世时就开始着手学习，在听了已爬过卫健委网站的同学痛斥了它反爬机制之麻烦，基于对自己的正确认知，我转战去爬了百度自带的每日疫情推送。但在花了两天第一次成功爬出数据后还在喜悦时，正式文档中的”直接爬取已经处理好的统计数据，满分为该模块分数上限的 40%”直接让我破防。

+ 然后就是开始无休止地“无用功 => 找到更好的方法重写！=>  进度阻塞遇到难以解决的bug => 无用功 ”的每天看起来都很忙，但每天都在做无用功的死循环。

+ 看不到成果的努力最容易挫伤一个人的自信，我开始焦虑自己最终只能拿到写博客的12分了，开始怀疑自己这门1学分的课我为什么要花这么多时间这么痛苦地去付出。看着舍友清闲地过着中秋节，我在图书馆忙碌一天后却碌碌无为，中间一度处于崩溃破防阶段。后来同学提供了他所用到的技术栈，让我有针对性地去学习，才让我逐渐从无头苍蝇的摸索中挣扎出来。

+ 虽然这两周很累，在崩溃破防了好多次后，经常想着算了，不然摆烂了，大不了就交一个只写着“努力了做不出来”的摆烂博客上去就算了。但看着身边同学们一步一步做出了成果，我还是存着不甘心硬着头皮做下去。但在慢慢地走走停停，渐渐地摸索以及询问身边人中，我逐渐也做出了一些看得见的东西，这让我更有动力也更有热情继续下去！

+ 但由于时间问题，用机器学习算法进行热点分析这一模块我来不及去学习和实操，但在ddl结束后我仍会尝试去将它做出来的！