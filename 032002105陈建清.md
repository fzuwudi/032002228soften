
# 一、PSP表格
| PSP2.1 | Personal SoftwareProcess Stages |预估耗时（分钟） |实际耗时（分钟)|
|--|--|--|--|
| Planning  | 计划 |60|60
| Estimate|· 估计这个任务需要多少时间|20|20
|Development|开发 |600|720
|· Analysis|· 需求分析 (包括学习新技术)|60|40
|· Design Spec|· 生成设计文档|30|30
|· Design Review|· 设计复审|15|10
|· Coding Standard|· 代码规范 (为目前的开发制定合适的规范)|10|10
|· Design|· 具体设计|60|60
|· Coding|· 具体编码|600|720
|· Code Review|· 代码复审|120|120
|· Test|· 测试（自我测试，修改代码，提交修改）|180|180
|Reporting |报告|60|60
|· Test Repor|· 测试报告|30|30
|· Size Measurement|· 计算工作量|20|20
|· Postmortem & ProcessImprovement Plan|· 事后总结, 并提出过程改进计划|60|60
||· 合计|1925|2140






# 二、任务要求的实现
 

## 1.项目设计与技术栈。

本次任务被我拆分成三个环节：对题意的理解与分析、执行完成作业任务、对作业调试与评估。通过和同学的探讨初步地得知该次作业要做的基本方向，通过网上学习和搜索资料，学习新的知识以编写代码程序，通过对生成数据的分析核对来验证爬取数据的正确性。
	
本次任务使用了python、http请求库requests等技术栈

## 2.爬虫与数据处理 。
通过requests库中的get函数获取有每天疫情信息统计页面信息文本，再通过xpath找到目标网页的src加上原网址的url便可以合成出新的每日详情页的网页，再通过get请求到新页面的详细数据，用beautifulsoup解析文本并用re正则函数找到需要的疫情信息，通过xlwings将数据写入excel

## 3.数据统计接口部分的性能改进。

	
在数据统计接口花费的时间大约有10小时，通过对各省份独立地查询，分别找出每个省份本土新增病例以及无症状感染病例的数据并以每行一个省份的详情数据的样式输出。通过先行定义没有新增病例省份的数据为0保证每个省份每种病例都可以有数据，以实现数据的统一。
	

## 4.每日热点的实现思路。

该功能因时间原因和个人能力原因没有实现

## 5.数据可视化界面的展示。

通过xlrd将数据绘制成柱状图![请添加图片描述](https://img-blog.csdnimg.cn/7cf9f4ef66bc47aeac320894c8a03a02.png)



# 三、心得体会
 通过这次作业，自学了python的众多知识，虽然因为各种原因，作业的完成度不高，但是也从中收获了很多。投入了很多的时间和精力在python基础和爬虫基础的学习之上，也通过实战获取到疫情信息，虽然作业并没有按预期全部完成，也因学到的东西比较基础而导致代码量大且冗余。但是成功爬取并导出爬取来的疫情信息写满excel上千行也是蛮有成就感的，希望通过日后继续努力学到更多东西来完善这次作业

