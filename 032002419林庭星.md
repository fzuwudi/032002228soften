## 2022软工K班个人编程任务
[作业Github链接](https://github.com/whhh22/032002419)
# 一、PSP表格
|PSP2.1|Personal Software Process Stages|预估耗时（分钟）|实际耗时（分钟）|
| :--| :-- | :-- | :-- |
Planning|计划|10|10
· Estimate|· 估计这个任务需要多少时间|10|10
Development|开发|2450|2585
· Analysis|· 需求分析 (包括学习新技术)|300|350
· Design Spec|· 生成设计文档|0|0
· Design Review|· 设计复审|0|0
· Coding Standard|· 代码规范 (为目前的开发制定合适的规范)|30|25
· Design|· 具体设计|60|60
· Coding|· 具体编码|2000|2000
· Code Review|· 代码复审|60|50
· Test|· 测试（自我测试，修改代码，提交修改）|100|100
Reporting|报告|15|15
· Test Repor|· 测试报告|0|0
· Size Measurement|· 计算工作量|5|5
· Postmortem & Process Improvement Plan|· 事后总结, 并提出过程改进计划|10|10
| |· 合计| 2475|2610

# 二、任务要求的实现
 - **项目设计与技术栈**
	 	这一次的任务被我拆分成四个环节，分别对应任务中的四个子任务，并在python环境下进行编程。其中，在数据获取环节中运用了爬虫技术，应用了requests库、lxml库；在数据统计环节中运用了正则匹配技术、数据存储技术，应用了re库、pandas库、openpyxl库；在每日热点环节中运用了数据提取、分析技术，应用了xlrd库；在数据可视化环节中运用了数据整合技术，应用了pyecharts库、xlrd库。
 - **爬虫与数据处理**
设计思路：首先，爬取指定网页中跳转到详情页面的所有链接，再分别爬取每个链接对应的页面内容，通过正则匹配筛选出所需的信息，再通过字符串匹配进一步提取出确切的值。
	爬虫部分设计了获取详情页链接的函数：
	```python
	def href_get(url, headers, href_list):
	    response_text = requests.get(url=url, headers=headers).text		# 爬取指定网页
	    tree = etree.HTML(response_text)
	    li_list = tree.xpath('//ul[@class="zxxx_list"]/li')		# 提取出<li>标签
	    for li in li_list:	
	        href = 'http://www.nhc.gov.cn' + li.xpath('./a/@href')[0]	  # 加上域名得到详情页链接
	        href_list.append(href)
	    return href_list
	```
	数据处理部分设计了提取数据的函数：
	```python
	def get_data_dic_values(match: tuple, data_dic: dict, g_a_t_cal_confirm_new: list):
	# match元组是由经正则表达式匹配后的字符串组成；
	# data_dic字典用于记录34个地区每日的新增确诊和无症状感染的数量；
	# g_a_t_cal_confirm_new列表用于记录新的一天港澳台各地区的新增确诊数量，以便与上一天的记录相减，获得新的一天的新增确诊数量
	    province_list = data_dic['Province']	
	    for province in province_list[0: 31]:	# 提取港澳台外的31个地区新增确诊的数量
	        str = match[1]
	        value = get_value(str, province)	# get_value()函数用于将字符串形式的数字转换成整型，下同
	        data_dic['newConfirm'].append(value)
	    for province in province_list:		# 提取新增无症状的数量
	        str = match[2]
	        value = get_value(str, province)
	        data_dic['newInfection'].append(value)   
	    for province in province_list[31:]:		# 单独记录港澳台地区累计确诊的数量
	        str = match[3]
	        value = get_value(str, province)
	        g_a_t_cal_confirm_new.append(value)
	    return data_dic, g_a_t_cal_confirm_new
	```
 - **数据统计接口部分的性能改进**
![在这里插入图片描述](https://img-blog.csdnimg.cn/4d469a32b62d43d28cb04d723f970df9.png#pic_center)	使用cProfile工具对数据处理中最复杂的函数（就是上一个代码块）进行了性能测试，如上图所示。开始以为哪出问题了，时间都是0.000，原来是我的函数运行太快了连0.001s都没有，难受了。。。最复杂的都这么“不堪”，其他就无需再测了，也没啥好改进的了，收工。
 - **每日热点的实现思路**
 有两种方法，一个是标准比较法，另一个是自身比较法。标准比较法是人为设定一个阈值，一旦超过则认为是值得注意的事件，即热点；自身比较法是以自身以往的数值为参考，若有所变化则认为是值得注意的事件，即热点。
结合以上两种方法，核心代码如下：
	```python
	for i in range(len(data_list)):
	# data_list数据结构层次为[[(province, new_confirm, new_infection), (), ...], [], ...]
	    date = date_list[i]	
	    new_confirm_up_list = []  # 记录新增确诊人数攀升的地区
	    new_confirm_hit_list = []  # 记录新增确诊人数众多的地区
	    new_infection_hit_list = []  # 记录新增无症状人数众多的地区
	    for j in range(34):	# 总共34个地区
	        province = data_list[i][j][0]
	        new_confirm = data_list[i][j][1]
	        new_infection = data_list[i][j][2]
	        if new_confirm >= hit_num:	# 如果超过人为设定的阈值hit_num，则添加为热点
	            new_confirm_hit_list.append(province)
	        if new_infection >= hit_num:
	            new_infection_hit_list.append(province)
	        if i != 0:
	            dt = new_confirm - data_list[i - 1][j][1]	# dt为今日新增确诊数量与昨日新增确诊数量之差
	            if dt > 0:	# 如果今天新增确诊数量比昨天多，则添加为热点
	                new_confirm_up_list.append(province)
	```
	所采用的算法较为通俗，能够直接地反映最新疫情变化的热点，但无法从一个较大的时间跨度来衡量疫情变化，缺少更宽广的视角。针对以上缺点，可以任取一段周期的数据作为样本，进行数据处理，如取平均数，计算方差等数学方法，得到具有代表性的参数，以此来衡量最新数据，得出结论。
 - **数据可视化界面的展示**
数据可视化的主要目的就是让数据直观、易懂地展示在观众眼前。由于要展示的数据多且密，所以常见的柱状图，折线图等等都不适合应用。既然选择以全国34个地区为对象，那自然而然会去想将这些地区的数据放置在一张中国地图上的对应位置去显示。为了进一步的视觉效果，根据不同地区的不同数值来填充不同深度的颜色。至此，地区名及其对应数据得到了展示，至于剩下的时间这一维度，选择引入一个时间轴，可以人为拖动到任意日期来查看当日的“疫情地图”，也可以点击时间轴旁的按钮来选择自动播放。
# 三、心得体会
从9.5大体了解任务的具体内容，到9.6学习爬虫，9.7正式创建工程，再到9.15基本完成编程任务，最后到9.17完成其余文本任务，提交作业。在这两个星期的时间，我狂学了许多东西，虽然不精，但是略懂一二，拓宽自己的知识储备，这对于我来说是很重要的。在大一寒假的时候，我也有收到过类似的任务，但是，那时的我不懂的太多，拿到一项任务不知题目是何意思，更不论从何解起。那时的我深感无奈，最后也选择了摆烂，烂在了心里。对比前后可以看出，我真的有在进步！这是让我最高兴的事，且不论任务完成得如何，能靠自己的能力去完成，我就已经战胜了过去的自己。这次任务也让我深深感受到，我要学的东西，要走的路还有很长很长，继续加油吧！
