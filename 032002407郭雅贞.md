@[TOC](2022软工K班个人编程任务
# 一、PSP表格

## (2.1)在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。
## (2.2)在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。
| PSP2.1 |Personal Software Process Stages | 预估耗时（分钟） | 实际耗时（分钟） |
| ------ | ------ | ------ | ------ |
| Planning | 计划 | 20 | 40 |
| Estimate | 估计这个任务需要多少时间 | 2400 | 2720 |
| Development | 开发 | 2240 | 2620|
| Analysis | 需求分析 (包括学习新技术) | 240 | 360 |
| Design Spec |  生成设计文档 | 10 | 20 |
| Design Review | 设计复审 | 10 | 10 |
| Coding Standard | 代码规范(为目前的开发制定合适的规范) | 20 | 30 |
| Design | 具体设计 | 30 | 60 |
| Coding | 具体编码 | 1440 | 2000 |
| Code Review | 代码复审 | 30 | 40 |
| Test | 测试（自我测试，修改代码，提交修改）|60 | 100 |
| Reporting | Reporting 报告 | 50 | 70 |
| Test Repor | 测试报告 | 30 | 40 |
| Size Measurement | 计算工作量 | 20 | 30 |
| Postmortem & Process Improvement Plan | 事后总结, 并提出过程改进计划| 40 | 60 |
| | 合计 | 2310 | 2730 |

# 二、任务要求的实现
## (3.1)项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方完成了各个环节？列出你完成本次任务所使用的技术栈。
首先是明确题目要求；接着是确定我要学什么新知识，在哪学，学多久，之前没有学过Python和爬虫，所以我在b站和CSDN寻找学习资料，但由于只有两周时间，很多内容是跳着看的，有很多内容还没有学到；然后就专门搜索关于疫情爬虫的资料，看看别人是怎么做的，有没有什么困难时事先可以避免的；之后就是仿照别人的思路自己打代码，遇到问题搜索解决，解决不了就...放弃。
本次任务中主要用到python，requests请求回复用于获取网页，html和js用于解析网页数据，matplotliib用于实现数据可视化。
## (3.2)爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。
主函数如下，用于获取基本的url和数据，并将数据传入函数中解析：

```python
def main():
    # 中国每日本土新增
    cn_url = "https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=chinaDayAddListNew"
    cn_response = requests.get(cn_url)
    cn_data = cn_response.json()['data']
    table_cn(cn_data)     #导入excel

    # 中国各省数据
    pro_url = "https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=localCityNCOVDataList,diseaseh5Shelf"
    pro_response = requests.get(pro_url)
    allpro_data = pro_response.json()['data']['diseaseh5Shelf']
    table_pro(allpro_data)     #导入excel

```
关于数据处理我主要分成了两个函数，一个用于解析全国数据，一个用于解析各省数据，导入Excel的部分直接在这两个函数中进行。
全国的数据与各省数据函数的区别主要在数据名称不同，其它地方相差不大，这里以各省的函数为例：由于港澳台没有历史数据，所以分开处理（我觉得这部分代码还可以再优化），一开始判断港澳台是写        if i == 0 or i == 1 or i == 31，台湾和香港排在前2，我看可以成功运行就爬了几个省份就关了，等到第二天爬全部的数据的时候，到只剩最后几个省份，我以为快成功了，又error了，回去看网页才发现每天省份的排行是会变的。

```python
def table_pro(items_pro):
    pro_data = []
    pro_data = items_pro['areaTree'][0]['children']
    j = 0
    # print("港澳台")
    app = xw.App(visible=True, add_book=False)
    wb = app.books.add()  # 打开Excel
    sht = wb.sheets['Sheet1']  # 建表
    sht.range('A1').value = '省份'
    sht.range('B1').value = '更新日期'  # lastUpdateTime
    sht.range('C1').value = '累计确诊'  # confirm
    sht.range('D1').value = '当日新增确诊'  # local_confirm_add
    sht.range('E1').value = '当日新增无症状'  # wzz_add
    for i in range(34):
        pro_name = pro_data[i]['name']  # 省份名字
        if pro_name == '台湾' or pro_name == '香港' or pro_name == '澳门':
            sht.range(f'A{j + 2}').value = pro_name
            pro_date = items_pro['lastUpdateTime'].split(' ')[0]  # 当前日期
            sht.range(f'B{j + 2}').value = pro_date
            pro_confirm = pro_data[i]['total']['confirm']  # 累计确诊
            sht.range(f'C{j + 2}').value = pro_confirm
            pro_confirm_add = pro_data[i]['today']['local_confirm_add']  # 当日新增确诊
            sht.range(f'D{j + 2}').value = pro_confirm_add
            pro_wzz = pro_data[i]['today']['wzz_add']  # 当日新增无症状
            sht.range(f'E{j + 2}').value = pro_wzz
            j = j + 1
        else:
            pro_adcode = pro_data[i]['adcode']  # 省份代码
            prohis_url = "https://api.inews.qq.com/newsqa/v1/query/pubished/daily/list?adCode=" + str(
                pro_adcode) + "&limit=130"   # 获取各省的url
            prohis_response = requests.get(prohis_url)
            prohis_data = prohis_response.json()
            table_prohis(prohis_data, app)   # 导入excel
            # print(prohis_data)
```
我爬取的是腾讯的数据，腾讯中各省数据存放的位置的url不同，需要获得各省的adcode后再获取新的url再获取数据，table_prohis(prohis_data)该函数用于解析除港澳台以外的各省数据。
```python
def table_prohis(prohis):
    prohis_data=prohis['data']
    app = xw.App(visible=True, add_book=False)
    wb = app.books.add()  # 打开Excel
    sht = wb.sheets['Sheet1']  # 建表
    sht.range('A1').value = '省份'
    sht.range('B1').value = '日期'  # date
    sht.range('C1').value = '当日新增确诊'  # confirm_add
    sht.range('D1').value = '当日新增无症状'  # wzz_add
    for i in range(130):
        items_prohis = prohis_data[i]
        pro_name = items_prohis['province']  # 省份
        sht.range(f'A{i + 2}').value = pro_name
        year = str(items_prohis['year'])
        month, day = str(items_prohis['date']).split('.')
        date = year + '-' + month + '-' + day
        sht.range(f'B{i + 2}').value = date
        pro_confirm_add = items_prohis['confirm_add']  # 当日新增确诊
        sht.range(f'C{i + 2}').value = pro_confirm_add
        pro_wzz = items_prohis['wzz_add']  # 当日新增无症状
        sht.range(f'D{i + 2}').value = pro_wzz
 
```
## (3.3)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。

## (3.4)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。
## (3.5)数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。
可视化部分的代码基本是看着别人的博客敲的，做了个很丑的折线图，考虑了很久之后还是决定把各省的图分开，这里以全国的数据和福建的数据为例：
全国：
![在这里插入图片描述](https://img-blog.csdnimg.cn/8daa4fe8f42243f281dd370b3260eb89.png#pic_center)

```python
def get_cn_img(data_cn):
    # data_cn = pd.read_csv("9.16全国数据.csv", encoding='gb2312')  # 导入csv文件
    plt.plot(data_cn["日期"], data_cn["当日新增确诊"], color='green', label='当日新增确诊')
    plt.plot(data_cn["日期"], data_cn["当日新增无症状"], color='blue', label='当日新增无症状')
    plt.title('2021.9.26至2022.9.16中国大陆本土疫情数据')
    # 图表的标题
    plt.xlabel('时间')
    # 图表x轴的标注
    plt.ylabel('人数')
    # 图表y轴的标注
    plt.legend()
    plt.show()

```

福建：
![在这里插入图片描述](https://img-blog.csdnimg.cn/92d0c280d81e4e3d8bfbcc7853e8d67c.png#pic_center)


福建部分的代码就是改一下上面的文件和title。

# 三、心得体会
## (4.1)在这儿写下你完成本次作业的心得体会，当然，如果你还有想表达的东西但在上面两个板块没有体现，也可以写在这儿~
 看完题目的第一反应：好难，我啥都不会，要从哪里开始......然后去找python跟爬虫的视频，一看时长，好像没法在两周内速成，那就直接看爬虫实例项目的视频吧。之后开始一步一步跟着视频实践，听说卫健委的反爬机制比较复杂，那先试试我能不能获取所有的url吧，结果只能获取第一页的信息，上网一搜只找到一篇关于卫健委反爬的，然而我看不懂他写的函数什么意思，之后我想要不先不管全部的数据了，先试试能不能获取一页的信息，然后就去学BeautiSoup，正则匹配，解析标签获取数据，成功获得了一天的数据，但是之前的问题还是解决不了，所以我知难而退，改去爬腾讯了.....（别人爬卫健委，我爬别人）
关于爬取腾讯数据的博客有很多，但是我一开始看到的博客对于数据接口的获取方式都讲的很模糊（后来我自己摸索出来后才让我刷到讲的很详细的orz），只知道通过开发者工具，那别人博客都直接把url放上去了，我就直接用吧，后来才知道接口跟数据格式是会变的，那就只能自己去f12里观察了，得到数据后发现腾讯只有近一年的数据，而且各省数据从今年5.10开始才是正常的（这个不知道是我的问题还是什么），但好歹有东西交了...
总结：这次作业对我来说十分困难！

