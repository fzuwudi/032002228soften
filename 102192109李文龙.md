GitHub链接：https://github.com/7daysss/102192109.git
# 一、PSP表格
## (2.1)在开始实现程序之前，在附录提供PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。（3'）
见附录
## (2.2)在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。（3'）
见附录
# 二、任务要求的实现
## (3.1)项目的设计与实现过程。用伪代码或流程图说明业务逻辑，简述代码的设计过程（比如有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明（12'）
 1. 爬虫名称
国内各省新冠肺炎疫情数据的爬取与可视化分析

2. 爬取内容与数据特征分析
爬取的内容包括了中国各省市新冠肺炎疫情数据。
爬取的数据都由中文和数字组成，所有数字数据都是大于等于0，不会出现小于0的情况。

3. 方案概述
分析网站页面结构，找到爬取数据的位置，根据不同的数据制定不同的爬取方法，将爬取的数据保存成csv文件，然后再将csv文件里的数据进行可视化处理。
 
 4. 网站html页面结构分析
从下面的网站html截图中可以看出，该网站由div标签进行分割内容，总共分了两大块，左边和右边，我们需要的数据在左边。
![在这里插入图片描述](https://img-blog.csdnimg.cn/9201916056784b57b399bcad58c31aa0.png)5.进一步分析网站的html，发现数据都在class=data-list的div标签里，在div标签里的列表标签ul，存放着中国每个省的疫情数据。
![在这里插入图片描述](https://img-blog.csdnimg.cn/26e849b3346c4a80bf7ba71ad7d20ced.png)6. 标签查找方法与遍历方法
通过上面网站html页面的分析，可以画出标签树。
![在这里插入图片描述](https://img-blog.csdnimg.cn/b79dc0b8d23b4891982da8618d12a713.png)
（1）地区名称所在标签的查找方法
各个省份的名称在class=list-pro-name的div标签里，而各省份的城市的名称在class=list-city-name的div标签里。因为这两个标签的class属性不同，所以可以用BeautifulSoup库的find()方法，利用标签属性值检索。
（2）地区疫情数据所在标签的查找方法
从上面的标签树中可以看出，省疫情数据和市疫情数据分别在不同的li标签下的div标签里，第二个li是第一个li的子标签。在这两个标签之间没有可以唯一标识的熟悉，所以要查找到这两个标签需要BeautifulSoup库的CSS选择器，通过标签的父子关系分别找到两个li标签。




## (3.2)数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（由VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。（10'）

性能分析图
![请添加图片描述](https://img-blog.csdnimg.cn/ee8d9fdc298b4ab19df5c6b08c56d13c.png)主要是对数据进行清洗与处理耗费的时间。
例如：

    try:
        dict['新增确诊人数'] = newConfirmed.group(1)
    except AttributeError:
        dict['新增确诊人数'] = 0

    try:
        dict['新增疑似人数'] = newSuspected.group(1)
    except AttributeError:
        dict['新增疑似人数'] = 0

    try:
        dict['新增死亡人数'] = newDead.group(1)
    except AttributeError:
        dict['新增死亡人数'] = 0

    try:
        dict['新增治愈人数'] = newCured.group(1)
    except AttributeError:
        dict['新增治愈人数'] = 0



1.保存数据
疫情数据爬取完后，通过Pandask库的to_csv()方法，将爬取后的数据保存为csv文件。
保存后的部分数据：
![在这里插入图片描述](https://img-blog.csdnimg.cn/8568681a8aae4617a8f8b905f1535797.png)
2. 数据清洗与处理
（1）因为全国疫情的数据量较大，所以我们可以通过pandas库来查看数据是否有异常、缺失、重复
（2）通过查看数据的简要信息，数据正常，数据的最小值也不是负数
（3）通过pandas库的isnull()方法查看是否有空值
（4）通过pandas库的duplicated()方法查看是否有重复行
（5）为了之后能更好的分析数据，需要对数据进行排序，后面再排序也可以
（6）保存处理后的数据

## (3.3)每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。（10'）
可以简单的生成各省新增确诊人数折线图，然后拟合曲线，计算疫情新增最多的时间点以及新增最少的时间点（导数为0），也可以找到新增确诊人数的拐点（二阶导数为0）。
优点：实现简单
缺点：参考价值有限。
改进方案：可以和舆情、新闻结合，进行多维度分析。

## (3.4)数据可视化界面的展示。在博客中详细介绍数据可视化界面的每一个组件和设计的思路。（10'）
中国新冠肺炎疫情地图
![在这里插入图片描述](https://img-blog.csdnimg.cn/84c4deb321824d7c94b01bcaa7269693.png).# 用pyecharts库画中国新冠疫情地图
from pyecharts import options as opts
from pyecharts.charts import Map
import pandas as pd
.# 自定义分段图例
pieces=[
        {"max": 70000, "min": 3000, "label": ">3000", "color": "#B40404"},
        {"max": 3000, "min": 1000, "label": "1000-3000", "color": "#DF0101"},
        {"max": 1000, "min": 100, "label": "100-1000", "color": "#F78181"},
        {"max": 100, "min": 10, "label": "10-100", "color": "#F5A9A9"},
        {"max": 10, "min": 0, "label": "<10", "color": "#FFFFCC"},
]
name = []
values = []
.# 导入数据
df = pd.DataFrame(pd.read_csv("各省的新冠肺炎疫情数据.csv"))
.# 处理数据，将数据处理成Map所要求的数据
for i in range(df.shape[0]):  # shape[0]:行数，shape[1]:列数
    name.append(df.at[i,'名称'])
    values.append(str(df.at[i,'累计确诊']))
total = [[name[i],values[i]] for i in range(len(name))]
.# 创建地图（Map）
china_map = (Map())
.# 设置中国地图
china_map.add("确诊人数",total ,maptype="china",is_map_symbol_show=False)
china_map.set_global_opts(
    # 设置地图标题
    title_opts=opts.TitleOpts(title="中国各省、直辖市、自治区、特别行政区新冠肺炎确诊人数"),
    # 设置自定义图例
    visualmap_opts=opts.VisualMapOpts(max_=70000,is_piecewise=True,pieces=pieces),
    legend_opts=opts.LegendOpts(is_show=False)
    )
.# 直接在notebook显示地图，默认是保存为html文件
china_map.render_notebook()


# 三、心得
## (4.1)在完成本次作业过程的心得体会（9'）
本次作业，我的评价是：超级难！
作为一个菜鸟，上手就是这种难度说实话有点难了。本次实验需要学习超多东西，例如：Python没学过得重新学，还有学习如何爬取网站内容，学习数据的输入输出，可视化，github的使用
所以本次实验做的很粗糙，能力有限，导致有些要求也没有完成。
当然也有一些收获，例如上面学习的东西，至少都上手了，至于精通还需要时间。

附录
PSP表格
![在这里插入图片描述](https://img-blog.csdnimg.cn/0f58fe747aaf433b84dc7b1f89c04a63.png)

