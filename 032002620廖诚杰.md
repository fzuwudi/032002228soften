GitHub 个人主页链接：https://github.com/jjjge?tab=repositories
# 一、PSP表格


| PSP2.1 | 预估耗时|实际耗时|
| ------ | ------ | ------ |
| 计划 | 0.5h |0.2h | 
| · 估计这个任务需要多少时间 | 0.5h |0.2h | 
| 开发 | 12.9h|17.3h | 
| · 需求分析 (包括学习新技术) | 5h |8h | 
| · 生成设计文档  | 0.5h |0.5h | 
| · 设计复审 | 0.2h |0.1h | 
| · 代码规范 (为目前的开发制定合适的规范) | 0.2h |0.2h | 
| · 具体设计 | 0.5h |1h | 
| · 具体编码| 5h |6h | 
| · 代码复审 | 0.5h |0.5h | 
| · 测试（自我测试，修改代码，提交修改） | 1h |1h | 
| 报告 | 2h |1h | 
| · 测试报告| 0.5h |0.5h | 
| · 计算工作量| 0.5h | 0.3h| 
| · 事后总结, 并提出过程改进计划 | 1h |0.2h | 
| · 合计 | 15.4h|18.5h | 
# 二、任务要求的实现

## 项目与技术栈

 本次作业我拆分成学习技术、编码、查错三个部分。在学习技术的过程中，我利用b站等平台学习了Python的语法，学会使用了re等库，并且学会了一些基础的爬虫知识；在编码时，由于对于Python掌握的不熟练和第一次接触爬虫的缘故，我遇到了很多困难，因此我在这过程中频繁地查阅资料以完成这部分要求；而在查错的部分则较为轻松，我采用核对法比对所得结果与预期结果是否一致来判断是否发生差错。、

## 爬虫与数据处理
爬虫的代码我分为两个部分：将疫情数据以txt格式爬到本地，将本地文本数据提取到Excel表格中。

![img](https://img-community.csdnimg.cn/images/399f1e9b01cd4874b7db23cc6891da6d.png "#left")

如图所示，获取txt的gettxt主要有三个函数，解析网页源码的fetchUrl，获取内容的getContent，和保存内容的saveFile。而提取数据getex则先将本土新增确诊和无症状的内容切片，再通过正则匹配的方式提取数据并记录到Excel表格中，代码如下

![img](https://img-community.csdnimg.cn/images/cda49d7c89074394904636a6795534bf.png "#left")
![img](https://img-community.csdnimg.cn/images/8c87f34f2eea450fb62cbce15289a82e.png "#left")
（仅展示部分内容，具体实现方法看我的GitHub仓库）
结果图

![img](https://img-community.csdnimg.cn/images/879c89b92eda444e97ec1ce006a2e382.png "#left")


![img](https://img-community.csdnimg.cn/images/083ab4422239417e84b7ad0283774d9f.png "#left")

![img](https://img-community.csdnimg.cn/images/3791b4dfa9b54276b1e03d7fc56244f1.png "#left")

## 数据统计接口部分的性能改进
性能分析采用的是time模块（不会用其他的，只能分析时间）
优化前

![img](https://img-community.csdnimg.cn/images/ceaeb57e584b4bc4b8696dd6a0348d0d.png "#left")

优化后

![img](https://img-community.csdnimg.cn/images/932c183d068245dc84e73921b694b1ff.png "#left")

## 每日热点的实现思路
每日热点的实现采用的是扫描每一天的新增确诊人数和新增无症状人数，超过一定数量则进行标记，且在保存内容在“hot.txt”文本中。（由于刚刚接触Python，且时间比较紧，对编码工具使用也不熟练，因此采用最笨的方法，此法得出的热点也不算特别的突出）

## 数据可视化界面的展示
数据可视化方面采用的是图表的形式，由于各省份的疫情情况数据量很大，因此仅展示2020年来全国的新增确诊和无症状情况

![img](https://img-community.csdnimg.cn/images/d4902af4521142e98500d99e5cd984f2.png "#left")

![img](https://img-community.csdnimg.cn/images/6162d7f105cd4a5d9abd9d111b2c04cf.png "#left")

2022新增确诊

![img](https://img-community.csdnimg.cn/images/b5cf340bdbef4c25a7d1d561d7ef2a08.png "#left")

![img](https://img-community.csdnimg.cn/images/922ea602adfb4172812f5f673719673c.png "#left")

![img](https://img-community.csdnimg.cn/images/33d1272a77244ce8a27ec0471fc5fabc.png "#left")

![img](https://img-community.csdnimg.cn/images/b7e67532278c404199c11828b8ea4ee9.png "#left")



# 三、心得体会

本次个人编程任务对我个人来说难度是比较大的，光是学习爬虫和Python就花去了我很大一部分时间，但是在爬取数据的过程中多少也学会了一些新的东西，爬文本数据时老是出错，不能爬取完整的数据，在这里很感谢龙哥大晚上来宿舍帮我debug。总的来说这次任务也是让我学会了很多的东西，也提升了我很多方面的能力。