- 我的作业Github链接：[](https://github.com/Edith129/mySpider)

# 一、PSP表格


| PSP2.1| Personal Software Process Stages | 预估耗时（分钟） | 实际耗时（分钟） |
| ------ | ------ | ------ | ------ |
| Planning | 计划 |  |  |
| · Estimate  | ·估计这个任务需要多长时间 | 40 | 40 |
| Development  | 开发 |  |  |
| · Analysis  | ·需求分析（包括学习新技术） | 1560 | 1740 |
| · Design Spec | ·生成设计文档 | 60 | 50 |
| · Design Review | ·设计复审 | 20 | 10 |
| · Coding Standard  | ·代码规格化（为目前的开发制定合适的规范） | 30 | 60 |
| · Design  | ·具体设计 | 50 | 90 |
| · Coding | ·具体编码 | 120 | 220 |
| · Code Review | ·代码复审 | 20 | 10 |
|· Test | · 测试（自我测试，修改代码，提交修改） | 40 | 130 |
| Reporting | 报告 |  |  |
| · Test Repor  | ·测试报告 | 40 | 30 |
| · Size Measurement  |  ·计算工作量 | 40 | 40 |
| · Postmortem & ProcessImprovement Plan | · 事后总结, 并提出过程改进计划 | 40 | 70 |
| 单元格 |  合计 | 2060 | 2490 |


# 二、任务要求的实现


**1. 项目设计与技术栈**

> 从阅读题目到完成作业，这一次的任务被我拆分成了6个环节，分别是进行爬虫，提取数据，进行数据分析，实现数据可视化，挖掘每日热点，最后在GitHub上建立库与文件夹将代码等上传。每个环节又分为学习与实践两个部分。其中爬虫，提取数据与对数据进行分析主要采用在B站上找课看来学习，并跟着课打代码，并根据自己的需求进行相应更改。其余部分主要在csdn与博客园上阅读相关博客进行阅读进行学习最后完成。在期间有遇到问题时，比如代码出错或者有相关问题不懂时也会寻求同学的帮助。完成本次任务主要学习并利用了asyncio，pyppeteer，lxml，os等知识。

**2.爬虫与数据处理**

> 在这部分的代码设计思路及过程如下：（1）首先利用get_page_url()函数来获取卫健委网站的疫情通报版面中每一页的url（共42页，其中每页中又有24条的具体通报新闻）；（2）获取到疫情通报版面的每页的url之后，利用get_page_html()函数对疫情通报版面每个页面进行html的抓取，由于卫健委利用了动态cookie，所以如果利用request等方法则会被反爬，所以利用了pyppeteer进行了抓取，且设置了延迟1s时间以模拟人类的比较真实的访问速度；（3）搞到疫情通报版面的html之后，分别对每页进行关键词的提取，首先是利用get_files_name()函数提取每个具体疫情情况通报的标题，存在一个列表当中，后利用get_news_url()函数对html中每条具体疫情情况通报的url进行提取，存在一个列表当中；（4）后利用get_page_html()函数分别抓取具体每条疫情情况页面的html；（5）再利用get_key_info()函数对该html中的疫情相关报道进行提取，保存需要数据；（6）后利用re与openpyxl对数据进行处理，留下自己需要的部分并保存为excel。

**3.数据统计接口部分的性能改进**

> 在爬的时候如果要将全部爬完，需要的时间太多了，应该是在解决动态cookie那个地方花的时间最多。具体测试与改进方法暂时还没做到。

**4.每日热点的实现思路**

> 思路是这个部分主要是对比每个省份前后两天新增人数的对比，当某地为低风险地区时先预设个阈值，当后一天的人数减去前一天的人数多于阈值时即判定为某地突发疫情；作为中高风险地区的人若新增人数比前一天多时，则不能判定为突发疫情，而是情况发展严峻；而当某地作为高风险地区时，若今天的新增人数比前一天少于某个阈值时则判定为重要转折。这种方法的实现优点在于简单，但是这种方法还是不够准确，要根据实际情况不断调整阈值的变化，随着时间的推移，可能对数量变化的要求越变越高，所以要根据实际情况进行调整。具体还没加入代码。

**5.数据可视化界面的展示**

> 实现思路是在该部分程序中利用了csv与pyecharts来将所获取的数据采用柱状图的方式进行数据可视化。首先是将文件做为csv形式的文件打开，打开之后利用csv.reader()函数再将自己需要的数据提取，后再利用冒泡排序将每个省份的每日新增病例进行排序，新增人数多的省份排列居前，最后利用bar1()函数描绘柱状图并生成html文件。但是具体实现还是有些问题，所以目前就暂时直接利用excel来生成图表。

[![2022-09-20-091343.png](https://i.postimg.cc/D0FVy22y/2022-09-20-091343.png)](https://postimg.cc/ygfpLCgw)


# 三、心得体会

> 这次的作业从一开始发布下来觉得是比较似曾相识的，觉得应该还是能做的不错的，但是在整个做的过程可谓是破防了一次又一次呢。整个作业流程下来说简单那是不太可能的，即使是前面有接触过爬虫也免不了从头再学一遍的任务。特别是爬卫健委这个网站实在是太要命了，使用动态的cookie的反爬机制与数据处理都让人直呼要大命了，每个地方都繁琐的很。但是在完成这个作业之后就会收获感满满and超有成就感！能爬下数据的那一瞬间那简直是快乐至极，觉得连卫健委都爬下来了，那就没什么自己爬不下来的网站了吧哈哈哈。而且在整个作业过程当中还是有对自己进行反思的，个人编程能力的不足与学了知识但是上手实在慢的问题到底该如何进行改进，查阅资料的能力是不是又有待加强，最后因为在反爬与数据分析那边花费了太多的时间导致后面有很多想法却没办法实现还是很难过的，希望下次能做的更好吧。另外现在作为一个信息的时代，爬虫对于个人的发展算是个加分项，在这次作业之后算是稍微掌握了这项技能，也算是个人有进步啦！

 