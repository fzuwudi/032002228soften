https://github.com/MyDejavu/041902205.git

# 一、PSP表格

| **PSP2.1**                               | **Personal Software             Process Stages** | 预计耗时 （分钟） | 实际耗时（分钟） |
| ---------------------------------------- | ------------------------------------------------ | ----------------- | ---------------- |
| Planning                                 | 计划                                             | 30                | 60               |
| · Estimate                               | · 估计这个任务需要多少时间                       | 30                | 30               |
| Development                              | 开发                                             | 600               | 720              |
| · Analysis                               | · 需求分析 (包括学习新技术)                      | 540               | 600              |
| · Design Spec                            | · 生成设计文档                                   | 60                | 60               |
| · Design Review                          | · 设计复审                                       | 60                | 60               |
| · Coding Standard                        | · 代码规范 (为目前的开发制定合适的规范)          | 60                | 60               |
| · Design                                 | · 具体设计                                       | 120               | 300              |
| · Coding                                 | · 具体编码                                       | 300               | 480              |
| · Code Review                            | · 代码复审                                       | 60                | 120              |
| · Test                                   | · 测试（自我测试，修改代码，提交修改）           | 60                | 120              |
| Reporting                                | 报告                                             | 60                | 90               |
| · Test Repor                             | · 测试报告                                       | 30                | 60               |
| · Size Measurement                       | · 计算工作量                                     | 60                | 60               |
| ·  Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划                   | 60                | 60               |
|                                          | · 合计                                           | 2070              | 2880             |

# 二、任务要求的实现

## （一）项目设计与技术栈

 1. ### 项目设计

    (1) 分析网页，选择需要使用到的爬虫技术：pyppeteer模拟用户操作浏览器、xpath解析数据

    (2) 从爬取到的数据中提取出所需的内容：正则表达式提取

    (3) 将提取出的内容进行解析，存储到Excel当中：openpyxl

    (4) 对数据进行每日热点分析与预测

    (5) 对数据进行可视化分析

 2. ### 技术栈

    python：pyppeteer、lxml、asyncio、openpyxl、os、re等

## （二）爬虫与数据处理

**需求分析**：根据题目的要求，统计中国大陆每日本土新增确诊人数及新增无症状感染人数，并且统计所有省份包括港澳台每日本土新增确诊人数及新增无症状感染人数

1. ### 爬虫 

   (1) **代码实现**：

   通过对网页目录部分的url进行观察，可以发现除了第一页以外，每一页的url都是有规律的以“_n”结尾，所以可以使用字符串拼接的方式生成每一页的url，在代码中使用了`def get_urllist()`来存储url列表

   ```markdown
   http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml
   http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_2.shtml
   http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_3.shtml
   ```

   通过对网页目录部分的源代码的格式进行解析，得到我们需要的url，通过`get_everydayhtml`()来获取每一页中的每一天疫情数据的url

   ``` html
    <li>  <a href="/xcs/yqtb/202209/6131c80a06b24352b0902395827b8eea.shtml" target="_blank" title='截至9月16日24时新型冠状病毒肺炎疫情最新情况'  >截至9月16日24时新型冠状病毒肺炎疫情最新情况</a><span class="ml">2022-09-17</span></li>
   ```

   使用`def get_filelist()`来获取并生成我们所爬取的数据存放时候的文件名

   使用`def get_content()`来获取我们所需要的最重要的疫情数据

   使用`def save_file()`将获取到的数据一一保存到相应的文件当中

   以上是对整个代码实现的一个结构的整理，从获取每一页的url，到获取目标界面的url，再到解析文件名称以及解析内容数据，最后将文件进行保存

   (2) **核心代码**：

   在爬虫的过程中发现这个网站是存在反爬机制的，除了基础的UA认证以外，还具有动态Cookie，一旦达到一定的访问次数，就会无法响应，如果使用普通的爬虫方法，如：`requests`，就需要手动的对Cookie进行修改才能继续访问。因此这里使用了`pyppeteer`，跟selenium类似，都是模仿用户访问浏览器的方式，具体可见：[Python 爬虫利器之 Pyppeteer 的用法 | 静觅 (cuiqingcai.com)](https://cuiqingcai.com/6942.html)

   ```python
   async def pyppeteer_main(url):
       browser = await launch({'dumpio': True})
       # newPage()相当于在浏览器中新建了一个选项卡
       page = await browser.newPage()
       # 绕过浏览器检测：evaluateOnNewDocument()，该方法是将一段 js 代码加载到页面文档中当发生页面导航、页面内嵌框架导航的时候加载的 js 代码会自动执行那么当页面刷新的时候该 js 也会执行，这样就保证了修改网站的属性持久化的目的
       await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                        '{ webdriver:{ get: () => false } }) }')
       # UA验证
       await page.setUserAgent(
           'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.27')
       # 访问主页
       await page.goto(url)
       # await asyncio.sleep(10)
       await asyncio.wait([page.waitForNavigation()])
       # 获取网页源码
       str = await page.content()
       await browser.close()
       return str
   ```

   [![QQ-20220917155721.png](https://i.postimg.cc/FHrdBzbL/QQ-20220917155721.png)](https://postimg.cc/qzFvtk5J)

   [<img src="https://i.postimg.cc/DysPTSbp/QQ-20220917155444.png" alt="QQ-20220917155444.png"  />](https://postimg.cc/kBJbNXsW)

2. ### 数据处理

   (1) **代码实现**：

   对于从文本中提取数据，需要使用到正则表达式，因此使用到`re`，并且需要使用到`openpyxl`将提取出来的数据存储到Excel当中

   由于仅需要统计本土病例的数据，使用`def get_localdata`获取本土病例

   使用`def verify_local`匹配文本中是否存在本土病例

   ```python
   def verify_local(content: str) -> str:
       if re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content) is None:
           return 0
       else:
           info = (re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content)).group()
           return info
   ```

   为了将数据保存在Excel当中，使用`def save_Excel`

   ```python
   # 将数据保存至Excel中
   def save_to_excel(data_1: dict, data_2: dict, filename):
       wb = Workbook()
       wb.remove(wb.active)    # 删除当前Workbook自带的sheet
       worksheet_1 = wb.create_sheet("新增本土病例")    # 在工作簿中新建一个sheet，用来保存本土病例数据
       worksheet_2 = wb.create_sheet("新增无症状感染者")   # 新建一个sheet，保存新增无症状数据
       excel_write(worksheet_1, data_1)    # 在sheet中写入本土病例数据
       excel_write(worksheet_2, data_2)    # 在另一个sheet中写入数据新增无症状数据
       wb.save(f'C:/Users/OSHMK/Desktop/wjwexce1/{filename}.xlsx') 
   ```
   
   [![QQ-20220917155702.png](https://i.postimg.cc/zG8KY0N3/QQ-20220917155702.png)](https://postimg.cc/gntnvynP)
   
   [<img src="https://i.postimg.cc/5yXQpR8k/QQ-20220917155426.png" alt="QQ-20220917155426.png"  />](https://postimg.cc/wRdBjFfh)

## （三）数据统计接口部分的性能改进

在开始的时候采用的是普通的爬虫方法，一条一条地进行爬取，用时接近两个半小时

[![299-F8-A03-E1-D9270-C6359-CE7-A4-EED2-D6-C.png](https://i.postimg.cc/Fzvh5c5X/299-F8-A03-E1-D9270-C6359-CE7-A4-EED2-D6-C.png)](https://postimg.cc/G8XWv4mX)

这个使用Pycharm平台的测试工具测试的测试结果图

[![2022-09-18-16-13-30.png](https://i.postimg.cc/Jn3YD71N/2022-09-18-16-13-30.png)](https://postimg.cc/G82x6CTt)

耗时最长的函数是使用`pyppeteer`获取网页源代码

```python
async def pyppeteer_main(url):
    browser = await launch({'dumpio': True})
    # newPage()相当于在浏览器中新建了一个选项卡
    page = await browser.newPage()
    # 绕过浏览器检测：evaluateOnNewDocument()，该方法是将一段 js 代码加载到页面文档中当发生页面导航、页面内嵌框架导航的时候加载的 js 代码会自动执行那么当页面刷新的时候该 js 也会执行，这样就保证了修改网站的属性持久化的目的
    await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                     '{ webdriver:{ get: () => false } }) }')
    # UA验证
    await page.setUserAgent(
        'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.27')
    # 访问主页
    await page.goto(url)
    # await asyncio.sleep(10)
    await asyncio.wait([page.waitForNavigation()])
    # 获取网页源码
    str = await page.content()
    await browser.close()
    return str
```

在同学的建议下，使用了CPU多线程（16线程）同时进行爬取，将时间缩短到了十多分钟

[![IMG-0568.jpg](https://i.postimg.cc/N0YyZx3b/IMG-0568.jpg)](https://postimg.cc/hXyPdVP7)

## （四）每日热点的实现思路

## （五）数据可视化界面的展示

由于时间问题以及自身技术力的限制，采取了直接通过Excel批量生成图表来做到每一日的数据可视化，分别展示了这三种可视化图表

**本土新增病例**

[![QQ-20220917162620.png](https://i.postimg.cc/3rPmw3Xy/QQ-20220917162620.png)](https://postimg.cc/qgxzbVZ0)

**无症状转确诊**

[![QQ-20220917163113.png](https://i.postimg.cc/026Qb4Gb/QQ-20220917163113.png)](https://postimg.cc/JGLmFP2L)

**本土新增无症状**

[![QQ-20220917163141.png](https://i.postimg.cc/6prBZx85/QQ-20220917163141.png)](https://postimg.cc/G4p00NPZ)

# 三、心得体会

首先是通过这次的编程作业，让我感受到了压力巨大，在没有开发经验的前提下，首先是迅速重温了一下python的语法，然后在哔哩哔哩通过视频学习了一下相关爬虫的一些知识，但由于卫健委网站的反爬机制比较强大，导致在b站上速成的技巧没办法直接使用（requests），无法直接套模板。刚开始并不清楚是什么反爬手段的问题，以为是什么代码bug，在查询和同学的告知下才知道无法用普通的方法爬取卫健委网站数据。

在同学的建议下，我尝试使用Pyppeteer这个模块，这个模块类似于Selenium，都是采取了模拟用户操作浏览器的逻辑，是一个非常好用的自动化测试工具。在使用了这个模块之后，成功解决了卫健委网站的反爬机制。同时需要用xpath（或bs4）对源代码进行解析，但是在解析过程中发现了网页的结构是非常不固定的，特别是疫情初发阶段，最后只能将文本部分进行一个整个的爬取。期间遇到了许多的问题，但好在在同学的帮助与提醒之下，逐渐是找到了方向，并且网上也有着许多教程可供参考，最终还是磕磕绊绊实现。

在进行数据分析与数据提取的时候，学习了正则表达式的规则以及如何使用正则表达式，如何使用`re`来对数据进行处理，并且使用`openpyxl`将数据导出到Excel当中，对如何进行数据提取有了初步的了解，感受到了正则表达式的强大之处。对于数据可视化，大概了解了关于`Pyecharts`的使用，但由于时间和技术力的原因，仅仅只是用了Excel进行了一个图表的生成，勉强可以看一下。

通过这次的个人编程任务，我感受到了自身编程能力的严重不足，需要花费更多的时间来跟上同学的进度，来努力完成作业。同时也明白了在遇到问题的时候要积极查看网络以及官方文档，并且可以向同学寻求帮助。
