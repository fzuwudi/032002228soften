# 一、PSP表格

|PSP2.1  |Personal Software Process Stages  |  预估耗时 |  实际耗时  | 
|--|--|--| --| 
|Planning|计划|30 |20 
|· Estimate|· 估计这个任务需要多少时间|30 |20 
|Development|开发|1250 |1500 
|· Analysis|· 需求分析 (包括学习新技术)|560| 720| 
|· Design Spec|· 生成设计文档|30 |20 
|· Design Review|· 设计复审|30 |20 
|· Coding Standard|· 代码规范 (为目前的开发制定合适的规范)|30 |20 
· Design|· 具体设计|30 |60
|· Coding|· 具体编码|480 |560
|· Code Review|· 代码复审|30 |30 
|· Test|· 测试（自我测试，修改代码，提交修改）|60 |60 
|Reporting|报告|90 |120 
|· Test Repor|· 测试报告|30 |60
|· Size Measurement|· 计算工作量|30 |20 
|· Postmortem & Process Improvement Plan|· 事后总结, 并提出过程改进计划|30 |40 
|--|合计|1370| 1640| 
|  |  |  |    | 






# 二、任务要求的实现

## 1、项目设计与技术化栈
		拿到任务就拆分成两部分，爬取数据和数据可视化。在B站、csdn、黑马程序员看视频，跟着写代码，看了很多资料综合起来、用到的技术栈：requests、re、json、csv、pandas、matplotlib、numpy、BeautifulSoup
## 2、爬虫与数据处理

```python
list_data = []
for case in caseList:
    item = {}
    item['area']=case['area']   #城市
    item['confirmedRelative']=case['confirmedRelative']  #新增
    item['asymptomaticRelative']=case['asymptomaticRelative']  #新增无症状
    item['confirmed']=case['confirmed'] #累计
    if item['asymptomaticRelative'] == '':  #这里港澳台的数据没有  所以判断为引号的话给它添加为0 方便后面分析
        item['asymptomaticRelative'] = 0
    list_data.append(item)  #把item这个字典添加到列表里面  因为pandas需要列表的数据
```
	本来用BeautifulSoup解析卫健委官网数据，成功提取出关键信息，但是因为反爬系统，偶尔才能爬取出数据，所以改成直接提取百度疫情数据，把json文件转成字典，然后提取里面的数据，利用不同项目名称筛选所需数据，并添加到列表中。最后再利用pandas把数据保存到ex表中。
	利用numpy库对提取出的数据进行处理，将处理出的结果利用matplotlib进行可视化分析（本来想用echarts进行大屏可视化分析的）。处理成柱状图。
	

```python
def bar_one(df):
    df = df.set_index('area')  #设置索引
    df.columns = ['确诊人数','无症状感染']  #给类型
    ax = df.plot.bar()  #bar柱状图
    plt.title('所有省份今日确诊')
    # plt.show()

def bar_two(df):
    df = df.set_index('area')  # 设置索引
    df = df.drop(['台湾','澳门','香港'])
    df.columns = ['确诊人数', '无症状感染']  # 给类型
    df.plot.bar()  # bar柱状图
    plt.title('所有省份今日确诊')
    plt.show()  #显示图片

if __name__ == '__main__':
    # todo  读表
    ex_path = r'C:\Users\Administrator\PycharmProjects\pythonProject\test1\demo_one.xlsx'  # ex表保存的绝对路径
    df = pd.read_excel(ex_path, index_col=False)   #固定写法  把ex表路径传进来  关闭索引
    df.drop(['Unnamed: 0', 'confirmed'], axis=1, inplace=True)   #因为读出来会多一个字段 所以把Unnamed字段删除
    bar(df)  #把df传到bar函数里面
    bar_one(df)  #把df传到bar_one函数里面
    bar_two(df)  #把df传到bar_two函数里面
```

## 3、数据统计接口部分的性能改进
	
		爬取数据
![在这里插入图片描述](https://img-blog.csdnimg.cn/cd1531fa729541aebcb9367a8f585926.png#pic_center)
可视化
![在这里插入图片描述](https://img-blog.csdnimg.cn/f3236e8c3b2a48f7bd1b891705619b0c.png#pic_center)





## 4、每日热点的实现思路
			爬取福建疫情有关内容，把细节展示出来。
## 5、数据可视化界面的展示
	
![在这里插入图片描述](https://img-blog.csdnimg.cn/f7c475b1acf14ee89f867aa10de434fc.png#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/7f14f663114a42dd9298996a2d8b55d6.png#pic_center)
因为台湾省的数据实在太大了，导致其他省份不能很好的显示在图表中，所以单独加了一张没有台湾的可视化数据。
![在这里插入图片描述](https://img-blog.csdnimg.cn/b69cfc59b7af4730a6a2d5b3f673583a.png#pic_center)


# 三、心得体会
	这次作业太有挑战了，之前没学过python，完全没接触过爬虫之类的，写起来十分痛苦。本来根据教程爬的挺顺利的，结果发现相同代码爬取数据时，有时候报错有时候能爬出所需信息，后面才知道卫健委的反爬系统不是简单加个headers就行的。官网一直动态刷新，cookie只能短时间有效，后面尝试了很多种方法都不行，只能放弃爬取卫健委官网的数据。其他网站爬取过程就很顺利了。进行数据可视化时，本来想用echarts进行大屏显示的，后来发现好像没有这么多数据来处理，就采取了matplotlib进行2D绘图。绘制出来发现确诊人数台湾遥遥领先，其他省份相比都约等于0了。最后基本完成了作业，虽然有好几天从早到晚写代码。写了改，改了删，最后还是挺有成就感的。
